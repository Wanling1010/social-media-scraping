{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary library\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import advertools as adv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bff587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d52903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('')\n",
    "\n",
    "# Remove timezone from columns\n",
    "df['date'] = df['date'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21581129",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6731b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for row in df.iterrows():\n",
    "    if 'x' in (row[1]['user']):\n",
    "        df_list.append(row[1])\n",
    "df = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df291c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_summary = adv.extract_emoji(df['rawContent'])\n",
    "emoji_summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_summary['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(emoji_summary['emoji_flat'][:10], emoji_summary['emoji_flat_text'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac52977",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_summary['top_emoji'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(facecolor='#ebebeb', figsize=(8, 8))\n",
    "plt.barh([x[0] for x in emoji_summary['top_emoji_text'][:20]][::-1],\n",
    "         [x[1] for x in emoji_summary['top_emoji_text'][:20]][::-1])\n",
    "plt.title('Top Emoji')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.gca().set_frame_on(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cfff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_summary = adv.extract_hashtags(df['rawContent'])\n",
    "hashtag_summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396479c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_summary['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_summary['top_hashtags'][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(facecolor='#ebebeb', figsize=(8, 12))\n",
    "plt.barh([x[0] for x in hashtag_summary['top_hashtags'][0:][:30]][::-1],\n",
    "         [x[1] for x in hashtag_summary['top_hashtags'][0:][:30]][::-1])\n",
    "plt.title('Top Hashtags')\n",
    "plt.grid(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ec1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_summary = adv.extract_mentions(df['rawContent'])\n",
    "mention_summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_summary['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e97115",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mention_summary['top_mentions'][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9299e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_summary = adv.extract_questions(df['rawContent'])\n",
    "question_summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_summary['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,x) for i, x in  enumerate(question_summary['question_text']) if x][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5986f",
   "metadata": {},
   "source": [
    "#### Frequency counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df.groupby(pd.Grouper(key='date', axis=0, \n",
    "                      freq='M')).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f29ae",
   "metadata": {},
   "source": [
    "#### Find Followers and Followings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d05789ff",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import tweepy\n",
    "import time\n",
    "#insert your Twitter keys here\n",
    "consumer_key =''\n",
    "consumer_secret=''\n",
    "access_token=''\n",
    "access_token_secret=''\n",
    "twitter_handle=''\n",
    "\n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "  \n",
    "# calling the api \n",
    "api = tweepy.API(auth)\n",
    "  \n",
    "# the screen_name of the targeted user\n",
    "screen_name = \"\"\n",
    "  \n",
    "# printing the latest 20 friends of the user\n",
    "for friend in api.friends(screen_name):\n",
    "    print(friend.screen_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d2d66ff",
   "metadata": {},
   "source": [
    "def get_following(screen_name):\n",
    "    print('Getting Following list of ',screen_name)\n",
    "    friends = []\n",
    "    friends_screenName = []\n",
    "    users = tweepy.Cursor(api.friends, screen_name='@'+screen_name,\n",
    "            wait_on_rate_limit=True,count=200)\n",
    "    for user in users.items():\n",
    "        try:\n",
    "            friends.append(user)\n",
    "            friends_screenName.append(user.screen_name)\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Going to sleep:\", e)\n",
    "            time.sleep(60)\n",
    "    print('Fetched number of following for '+screen_name+' : ',len(friends))            \n",
    "    return friends,friends_screenName"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13d4cb89",
   "metadata": {},
   "source": [
    "def get_followers(screen_name):\n",
    "    print('Getting Follower list of ',screen_name)\n",
    "    followers = []\n",
    "    followers_screenNames = []\n",
    "    users = tweepy.Cursor(api.followers, screen_name='@'+screen_name, wait_on_rate_limit=True,count=200)\n",
    "    for user in users.items():\n",
    "        try:\n",
    "            followers.append(user)\n",
    "            followers_screenNames.append(user.screen_name)\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Going to sleep:\", e)\n",
    "            time.sleep(60)\n",
    "    \n",
    "    print('Fetched number of followers for '+screen_name+' : ',len(followers))\n",
    "    return followers,followers_screenNames"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b727c51",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def fetch_data():\n",
    "    people = list(get_followers(''))[0]\n",
    "    people.extend(list(get_following('')[0]))\n",
    "    data = {}\n",
    "    for user in people:\n",
    "        try:\n",
    "            print('Fetching data of ',user.name)\n",
    "            ob = {\n",
    "                    'ID':user.id,\n",
    "                    'Name':user.name,\n",
    "                    'StatusesCount':user.statuses_count,\n",
    "                    'Follower_Count':user.followers_count,\n",
    "                    'Following_Count':user.friends_count,\n",
    "                    'Followers':list(get_followers(user.screen_name))[1],\n",
    "                    'Following':list(get_following(user.screen_name))[1],\n",
    "            }\n",
    "            print(ob)\n",
    "            data[user.screen_name].append(ob)\n",
    "            print('Data saved, moving on to next user\\n\\n')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            pass\n",
    "    \n",
    "    with open('data.txt', 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9518527",
   "metadata": {},
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282585c",
   "metadata": {},
   "source": [
    "#### Perform Iterations of the word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2: GENERATING WORD CLOUD AFTER TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = df['tweets_clean_lemmatized'].tolist()\n",
    "\n",
    "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Transform the preprocessed corpus into a TF-IDF matrix\n",
    "tdm = vectorizer.transform(corpus)\n",
    "\n",
    "# Print the resulting matrix\n",
    "tf_idf_matrix = tdm.toarray()\n",
    "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to convert into dictionary\n",
    "def Convert(tup, di):\n",
    "    di = dict(tup)\n",
    "    return di\n",
    "dictionary = {}\n",
    "freqs = [(word, tdm.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "d = Convert(freqs, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WordCloud(mode='RGBA',\n",
    "              stopwords=stop_words,\n",
    "              background_color='white',\n",
    "              max_words=1000, \n",
    "              height = 2000, \n",
    "              width=4000, \n",
    "              font_path=fp).generate_from_frequencies(d)\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.imshow(w) \n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(d.items(), key=lambda x: (-x[1], x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632fc42",
   "metadata": {},
   "source": [
    "#### Look at the EN tweets he made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb11134",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets = df[df['lang'] == 'en']\n",
    "len(en_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = en_tweets['tweets_clean_lemmatized'].tolist()\n",
    "\n",
    "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(en_corpus)\n",
    "\n",
    "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Transform the preprocessed corpus into a TF-IDF matrix\n",
    "tdm = vectorizer.transform(en_corpus)\n",
    "\n",
    "# Print the resulting matrix\n",
    "tf_idf_matrix = tdm.toarray()\n",
    "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dictionary = {}\n",
    "freqs = [(word, tdm.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "d = Convert(freqs, en_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WordCloud(mode='RGBA',\n",
    "              stopwords=stop_words,\n",
    "              background_color='white',\n",
    "              max_words=1000, \n",
    "              height = 2000, \n",
    "              width=4000, \n",
    "              font_path=fp).generate_from_frequencies(d)\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.imshow(w) \n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb39e7",
   "metadata": {},
   "source": [
    "References: \n",
    "- https://medium.com/geekculture/how-to-extract-reddit-posts-for-an-nlp-project-56d121b260b4\n",
    "- https://praw.readthedocs.io/en/stable/tutorials/comments.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda92a37",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21d7e6f1",
   "metadata": {},
   "source": [
    "!pip install visdcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94932da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('', sheet_name='Sheet2')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef0983",
   "metadata": {},
   "source": [
    "#### Change the orientation of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01796b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['Source'] = df2[df2.columns[0:2]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)), \n",
    "    axis=1\n",
    ")\n",
    "df2['Target'] = df2[df2.columns[3:5]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62fbcb",
   "metadata": {},
   "source": [
    "#### Plot the network for following/followers on twitter\n",
    "\n",
    "#### Method 1: Pyvis Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78430072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.from_pandas_edgelist(df2, source='Source', target='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b776aae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True)\n",
    "net.from_nx(G)\n",
    "net.show('example.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215e702",
   "metadata": {},
   "source": [
    "#### Mentions map"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7231717",
   "metadata": {},
   "source": [
    "!pip3 install twint\n",
    "\n",
    "# repo has been archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_temp = df['mention'].str.split(',').explode('mention').value_counts()\n",
    "mention_temp_df = pd.DataFrame(mention_temp).reset_index()\n",
    "mention_temp_df = mention_temp_df.rename(columns={'index':'target', 'mention':'mention_counts'})\n",
    "mention_temp_df['source'] = ''\n",
    "mention_temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G_nx = nx.from_pandas_edgelist(mention_temp_df, source='source', target='target')\n",
    "\n",
    "# saving graph created above in gexf format\n",
    "#nx.write_gexf(G_nx, \"mentions_map.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True)\n",
    "net.from_nx(G_nx)\n",
    "net.show('example_mentions.html')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e229ab8d",
   "metadata": {},
   "source": [
    "# plotting the mentions map using networkx\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True)\n",
    "net.from_nx(G_nx)\n",
    "net.show('example.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6629a0",
   "metadata": {},
   "source": [
    "#### Method 2: Plotly Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a48604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import visdcc\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create app\n",
    "app = dash.Dash()\n",
    "node_list = list(set(df2['Source'].unique().tolist() + df2['Target'].unique().tolist()))\n",
    "nodes = [{'id': node_name, 'label': node_name, 'shape': 'dot', 'size': 7} \n",
    "        for i,node_name in enumerate(node_list)]\n",
    "\n",
    "# create edges from df\n",
    "edges = df2\n",
    "for row in df2.to_dict(orient='records'):\n",
    "    source, target = row['Source'], row['Target']\n",
    "    edges.append({\n",
    "        'id': source + \"_\" + target, \n",
    "        'from': source,\n",
    "        'to': target, \n",
    "        'width': 2\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "# define layout\n",
    "app.layout = html.Div([visdcc.Network(id='net', data={'nodes': nodes, 'edges': edges}, options=dict(height='600px',width='100%')),\n",
    "                      dcc.RadioItems(id='color', \n",
    "                                     options=[{'label': 'Red', 'value':'#ff0000'}, \n",
    "                                             {'label': 'Green', 'value':'#00ff00'}, \n",
    "                                             {'label': 'Blue', 'value':'#0000ff'}], \n",
    "                                    value='Red')])\n",
    "# define callback\n",
    "@app.callback(Output('net', 'options'), \n",
    "             [Input('color', 'value')])                       \n",
    "def myfunc(x):\n",
    "    return {'nodes': {'color': x}}\n",
    "# define main calling\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff3f32",
   "metadata": {},
   "source": [
    "## TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b3629",
   "metadata": {},
   "source": [
    "#### METHOD 1: BERTopic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0d3ffc3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "tweets_df = pd.read_excel('data/twitter/tweets_df.xlsx')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ec0188f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# we add this to remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language='english', calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "topics, probs = model.fit_transform(df_tweets['Tweets_Eng'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef2df4",
   "metadata": {},
   "source": [
    "#### METHOD 2: TweetNLP\n",
    "\n",
    "\n",
    "Refer to Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest category for each row\n",
    "cols = ['arts_&_culture', 'business_&_entrepreneurs', 'celebrity_&_pop_culture', 'diaries_&_daily_life', 'family', 'fashion_&_style', 'film_tv_&_video', 'fitness_&_health', 'food_&_dining', 'gaming', 'learning_&_educational', 'music', 'news_&_social_concern', 'other_hobbies', 'relationships', 'science_&_technology', 'sports', 'travel_&_adventure', 'youth_&_student_life']\n",
    "\n",
    "df['Topic Classification'] = df[cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the biggest sentiment for each row\n",
    "sentiment_cols = ['negative', 'neutral', 'positive']\n",
    "df['Sentiment Analysis'] = df[sentiment_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30175941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irony detection for each row\n",
    "irony_cols = ['non_irony', 'irony']\n",
    "df['Irony Detection'] = df[irony_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f887b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hate detection for each row\n",
    "hate_cols = ['NOT-HATE', 'HATE']\n",
    "df['Hate Detection'] = df[hate_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "445009ae",
   "metadata": {},
   "source": [
    "import ast\n",
    "from ast import literal_eval\n",
    "\n",
    "# For column 0\n",
    "# fillna\n",
    "df['NER_8'] = df['NER_8'].fillna('{}')\n",
    "# convert the column to dicts\n",
    "df['NER_8'] = df['NER_8'].apply(literal_eval)\n",
    "# use json_normalize\n",
    "df = df.join(pd.json_normalize(df.pop('NER_8')),lsuffix='_7', rsuffix='_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 75:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47057202",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Tweets_Eng', 'date', 'type_1', 'entity_1', 'probability_1', 'type_2', 'entity_2', 'probability_2', 'type_3', 'entity_3', 'probability_3', 'type_4', 'entity_4', 'probability_4', 'type_5', 'entity_5', 'probability_5', 'type_6', 'entity_6', 'probability_6', 'type_7', 'entity_7', 'probability_7', 'type_8', 'entity_8', 'probability_8', 'type_9', 'entity_9', 'probability_9']\n",
    "filtered_df = df[cols]\n",
    "filtered_df['id'] = filtered_df.index\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = pd.wide_to_long(\n",
    "    df.reset_index(), i=[\"id\", \"renderedContent\"], j=\"value\", stubnames=[\"type\", \"entity\", \"probability\"], sep = \"_\", suffix=\"\\d+\"\n",
    ")\n",
    "x.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7f5b0",
   "metadata": {},
   "source": [
    "#### METHOD 3: ZERO SHOT CLASSIFICATION\n",
    "\n",
    "Part 1: English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets_Eng'] = df['Tweets_Eng'].astype(str)\n",
    "df['Tweets_Eng'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. REMOVE PUNCTUATION\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "df['Tweets_Eng_Clean'] = df['Tweets_Eng'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "# 2. LOWER CASING\n",
    "df['Tweets_Eng_Clean'] = df['Tweets_Eng_Clean'].apply(lambda x: x.lower())\n",
    "\n",
    "# 3. REPLACE CONTRACTIONS\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "df['Tweets_Eng_Clean'] = df['Tweets_Eng_Clean'].apply(lambda x: replaceContraction(x))\n",
    "\n",
    "# 3. TOKENIZATION\n",
    "import re\n",
    "def tokenization(text):\n",
    "    tokens = re.split('W+',text)\n",
    "    return tokens\n",
    "\n",
    "df['Tweets_Eng_Clean'] = df['Tweets_Eng_Clean'].apply(lambda x: tokenization(x))\n",
    "\n",
    "# 4. REMOVE STOPWORDS\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "df['Tweets_Eng_Clean'] = df['Tweets_Eng_Clean'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "# 5. LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "\n",
    "df['Tweets_Eng_Clean'] = df['Tweets_Eng_Clean'].apply(lambda x:lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2db718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets_Eng_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for row in df.iterrows():\n",
    "    if '' in (row[1]['user']):\n",
    "        df_list.append(row[1])\n",
    "df = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_labels\n",
    "labels = [\"Military\", \"Healthcare\", \"Nation Building\", \"Politics\", \"Economy\", \"Food\", \"Volunteer\", \"Bilateral Meeting\", \"Education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596516da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Zero-Shot Classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "#classifier(df['Tweets_Eng_Clean'][0], labels, multi_label=True) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "845e0253",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Creating a function to call the ZSTC iteratively for all rows\n",
    "def zeroshot(dataset, labels):\n",
    "    \"\"\"\n",
    "    This function takes in a dataset with a text column and the corresponding support labels of the standardized label.\n",
    "    The Zero-Shot Topic Classification algoritdf will determine a confidence score for each support label.\n",
    "    The corresponding confidence score given by the model will be added as a new column to the original dataset.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    all_dic = []\n",
    "    for i in df['Tweets_Eng_Clean']:\n",
    "        result.append(classifier(i, labels, multi_label=True))\n",
    "    for j in range(len(result)):\n",
    "        dic = {result[j][\"labels\"][i]: result[j]['scores'][i] for i in range(len(result[j]['scores']))}\n",
    "        sorted_tuples = sorted(dic.items(), key=lambda item: item[1], reverse=True)\n",
    "        all_dic.append(dict(sorted_tuples))\n",
    "    return pd.concat([dataset, pd.DataFrame(all_dic)], axis=1)\n",
    "\n",
    "# apply the zero shot topic classification algoritdf\n",
    "df2 = zeroshot(df, labels)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21425970",
   "metadata": {},
   "source": [
    "Part 2: Native Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fa7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('ប្រសាសន៍ ឯកឧត្តម បណ្ឌិត ហ៊ុន ម៉ាណែត ថ្លែងក្នុងពិធីសម្ពោធដាក់ឱ្យប្រើប្រាស់ជាផ្លូវការស្ពានបេតុងចំនួន ៩', labels, multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c0032",
   "metadata": {},
   "source": [
    "#### METHOD 4: GSDMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b471a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = result\n",
    "\n",
    "# be sure to split sentence before feed into Dictionary\n",
    "dataset = [d.split() for d in docs]\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(dataset)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in dataset]\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=15, alpha=0.1, beta=0.3, n_iters=15)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(dataset, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c8aa8",
   "metadata": {},
   "source": [
    "# VISUALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831fab5",
   "metadata": {},
   "source": [
    "Part 1: TweetNLP Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39066b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.get_loc(\"youth_&_student_life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetnlp_topic = df.iloc[:, 36:55]\n",
    "cols = tweetnlp_topic.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_max(row, cols):\n",
    "    return cols[np.argmax(row.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic Classification'] = tweetnlp_topic.apply(lambda x: return_max(x,cols), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['Topic Classification'].value_counts().rename_axis('Topic').reset_index(name='count')\n",
    "counts['count'] = counts['count'].astype(int)\n",
    "ax = sns.barplot(y=counts['Topic'], x=counts['count'], data=counts, orient='h')\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30,20)})\n",
    "sns.histplot(data=df, y='Topic Classification', discrete=True, legend=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "715a0a0e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "s = df['Topic Classification'].value_counts(ascending=False)           # compute counts by class\n",
    "ax = sns.barplot(x=s.values, y=s.index, order=s.index) # plot count plot\n",
    "ax.set(xlabel='class', yticks=[], title='Number of survivors by class', frame_on=False) # prettify\n",
    "# ax.tick_params(length=0)                               # remove tick liens\n",
    "ax.bar_label(ax.containers[0]);                        # add bar labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f42e49",
   "metadata": {},
   "source": [
    "Part 2: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a024d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['Sentiment Analysis'].value_counts().rename_axis('Sentiments').reset_index(name='count')\n",
    "counts['count'] = counts['count'].astype(int)\n",
    "ax = sns.barplot(y=counts['Sentiments'], x=counts['count'], data=counts, orient='h')\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ae33d",
   "metadata": {},
   "source": [
    "Perform word cloud on entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225369ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = df['entity'].astype(str).tolist()\n",
    "entity_list = [s.strip() for s in entity_list]\n",
    "entity_list = [x for x in entity_list if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9123337e",
   "metadata": {},
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e42ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stop_words = ['amp'] + list(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = \" \".join(text for text in entity_list) \n",
    "fp = 'font/gargi.ttf'\n",
    "wordcloud = WordCloud(stopwords=stop_words,\n",
    "                      background_color = 'white', \n",
    "                      max_words=1000, \n",
    "                      height = 2000, \n",
    "                      width=4000, \n",
    "                      font_path=fp).generate(words) \n",
    "plt.figure(figsize = (16,8))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
