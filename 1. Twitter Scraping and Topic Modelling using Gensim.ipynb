{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec678d7",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66443f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# wordcloud\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# Expand column and row sizes \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc34e47",
   "metadata": {},
   "source": [
    "## 1. Scrape twitter and translate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dba0a8cc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Created a list to append all tweet attributes(data)\n",
    "attributes_container = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('').get_items()):\n",
    "    if i>1000000:\n",
    "        break\n",
    "    attributes_container.append([tweet.lang, tweet.url, tweet.date, tweet.id, tweet.user.username, tweet.likeCount, tweet.sourceLabel, tweet.rawContent, tweet.replyCount, tweet.retweetCount, tweet.outlinks, tweet.tcooutlinks, tweet.quoteCount, tweet.mentionedUsers,\n",
    "                                tweet.retweetedTweet, tweet.quotedTweet, tweet.inReplyToUser, tweet.coordinates, tweet.place])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"Language\", \"Tweet Location\", \"Date Created\", \"Tweet ID\", \"Username\", \"Number of Likes\", \"Source of Tweet\", \"Tweets\", \"Count of replies\", \"Count of retweets\", \"Outlinks\", \"Tcooutlinks\", \"Count of users quoted the tweets and replied\",\"User objects of mentioned users\"\n",
    "                                                       ,\"Retweeted Tweet\", \"Quoted Tweet\", \"In Reply To User\", \"Coordinates\", \"Place\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12e9be14",
   "metadata": {},
   "source": [
    "# Check which columns have timezones datetime64[ns, UTC] \n",
    "tweets_df.dtypes\n",
    "\n",
    "# Remove timezone from columns\n",
    "tweets_df['Date Created'] = tweets_df['Date Created'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00061cfb",
   "metadata": {},
   "source": [
    "tweets_df.to_excel('tweets_df.xlsx', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc302d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_excel('tweets_df.xlsx')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bddc5",
   "metadata": {},
   "source": [
    "## 2. Prepare stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','m', 'the', 'of', 'https', 'itb', 'htt', 'http', 'wat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17716580",
   "metadata": {},
   "source": [
    "## 3. Remove urls and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29401d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_zwsp(text):\n",
    "    text.replace('\\u200b', '').replace('\\n', '').replace(' ', '')\n",
    "    return text\n",
    "\n",
    "df['Tweets'] = df['Tweets'].apply(lambda x: replace_zwsp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.Tweets.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "data = [sent.lower() for sent in data]\n",
    "\n",
    "data = [re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,string\n",
    "\n",
    "def strip_links(text):\n",
    "    \n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links \n",
    "data = [strip_links(re.sub(r'http\\S+', '', sent)) for sent in data]\n",
    "\n",
    "# strip entities\n",
    "data = [strip_all_entities(sent) for sent in data]\n",
    "\n",
    "# remove emojis \n",
    "import emoji\n",
    "list_emojis=[]\n",
    "\n",
    "for d in emoji.emoji_list(data):\n",
    "    list_emojis.append(d['emoji'])\n",
    "    data = re.sub('|'.join(list_emojis), '', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb829cb4",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2d582d6",
   "metadata": {},
   "source": [
    "# from deep_translator import GoogleTranslator\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc9228de",
   "metadata": {},
   "source": [
    "# translate data\n",
    "translator = Translator()\n",
    "tweets_df['Tweets_Eng'] = tweets_df['Tweets'].apply(lambda x: translator.translate(x, dest='en').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df9f4d",
   "metadata": {},
   "source": [
    "## 4. Tokenise words, remove punctuations and unnecessary characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f42c499",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 5. Creating Bigrams and Trigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e258bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78933fb",
   "metadata": {},
   "source": [
    "## 6. Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821760c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244568b4",
   "metadata": {},
   "source": [
    "Let's call the functions in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354aafb6",
   "metadata": {},
   "source": [
    "## 7. Create dictionary and corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5e5c1",
   "metadata": {},
   "source": [
    "Create human readable form of the corpus itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9da21",
   "metadata": {},
   "source": [
    "## 8. Building the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b07a8",
   "metadata": {},
   "source": [
    "## 9. View topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbaf341",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b0f75",
   "metadata": {},
   "source": [
    "## 10. Check Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence score shows how good topic models are\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4185f",
   "metadata": {},
   "source": [
    "## 11. Visualise topic-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824615f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40274aa",
   "metadata": {},
   "source": [
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73ec98",
   "metadata": {},
   "source": [
    "## 12. Generate Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ea58c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ''\n",
    "\n",
    "for lst in data_lemmatized:\n",
    "    for word in lst:\n",
    "        text += (word + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated image:\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(text)\n",
    "# Open a plot of the generated image.\n",
    "\n",
    "plt.figure( figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f8f84",
   "metadata": {},
   "source": [
    "## 13. Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens = [t for t in text.split()]\n",
    "freqs = nltk.FreqDist(tokens)\n",
    "freq_list = [(k, v) for k, v in freqs.items()]\n",
    "words_list = []\n",
    "frequencies_list = []\n",
    "\n",
    "for tup in freq_list:\n",
    "    word = tup[0]\n",
    "    freq = tup[1]\n",
    "    words_list.append(word)\n",
    "    frequencies_list.append(freq)\n",
    "\n",
    "res = {'words': words_list, \n",
    "      'frequencies': frequencies_list}\n",
    "res = pd.DataFrame.from_dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc636bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.sort_values('frequencies', ascending=False)\n",
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c70e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res.to_csv('word_frequencies_others.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c7537",
   "metadata": {},
   "source": [
    "## 14. Find Dominant Topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12620e7f",
   "metadata": {},
   "source": [
    "## 15. Find the most representative sentence in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a19f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorted = pd.DataFrame()\n",
    "\n",
    "sent_topics_out_gb = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_out_gb:\n",
    "    sent_topics_sorted = pd.concat([sent_topics_sorted, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorted.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3773e4",
   "metadata": {},
   "source": [
    "## 16. Topic distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea60c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the volume and distribution of topics to judge how widely it was discussed\n",
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f585ca2",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6209bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "df[[\"polarity\", \"subjectivity\"]] = df[\"Tweets_Eng\"].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "\n",
    "for index, row in df[\"Tweets_Eng\"].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score[\"neg\"]\n",
    "    neu = score[\"neu\"]\n",
    "    pos = score[\"pos\"]\n",
    "    comp = score[\"compound\"]\n",
    "    if neg > pos:\n",
    "        df.loc[index, \"sentiment\"] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        df.loc[index, \"sentiment\"] = \"positive\"\n",
    "    else:\n",
    "        df.loc[index, \"sentiment\"] = \"neutral\"\n",
    "        df.loc[index, \"neg\"] = neg\n",
    "        df.loc[index, \"neu\"] = neu\n",
    "        df.loc[index, \"pos\"] = pos\n",
    "        df.loc[index, \"compound\"] = comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=[\"Total\",\"Percentage\"])\n",
    "\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(df, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947daf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pc = count_values_in_column(df, \"sentiment\")\n",
    "names= pc.index\n",
    "size=pc[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color=\"white\")\n",
    "plt.pie(size, labels=names, colors=[\"green\", \"blue\",\"red\"])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e893d38f",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ba342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
